hello everyone uh welcome back to my channel so I recently hit a hundred subscribers so thank you so much for your support if you have not subscribed me yet so I will invite you to come and join with me so from this video we are going to build a q a system on YouTube video so basically the idea is get YouTube url convert it into an audio file and then extract the text transcription from this video and then build the Q a model so if you feel boring to listen let the lectures tutorials and want to summarize podcasts Etc and also what q a with videos that you are watching or watched so this is something you should watch not necessarily to be YouTube videos with a small Amendment you can use this same tutorial for the other videos or audio files so one of the practical application would be summarizing tons of call recordings in a call center and do intelligent analysis on those with chat GPT so without much talking uh let's move to the notebook okay so uh first we need to install the libraries uh here we will be installing the whisper library from open AI so this is basically we are using to transcribe the audio file and then we will use the pi tube so this library is being used to a lot YouTube videos to the collab notebook then as always we will load the land chain chroma DB and open AI as the vector DB we will be using chroma DB which is an open source Vector storage next we can import the libraries that we are using for this notebook so basically the whisper and also we need to import torch Library so in this case we will be using the GPU support in collab environment so we need torch to enable the GPU support and also YouTube from the pi tube library and various text Splitters document loaders Vector stores chains embeddings llms from the blank chain framework and also uh here we will we needed pandas Library also to handle the transcription uh data frames next we can Define the device so here I will be uh Define the device if there is the GPU support to make it uh use the GPU support like this so when you are implementing this in your collab environment make sure to go to runtime and from the change runtime type make sure to enable the GPU Hardware accelerator support next we can initiate the whisper model so here from the whisper.lord model we can load the model and set the device as a GPU here and also here we have several model variations with the whisper so here I will be using the live so if you if your environment is not supported with the large visible model you can use the base model and Etc so uh always you can use this code in your local computer with a better GPU so if you have a better GPU always make sure to use the best model you have with the whisper all right next I'm defining this function so basically the task of this function is to get a URL the YouTube URL and then extract from the this URL get the YouTube video and then from that filter the audio part of the that particular video and then save it as an MP3 file in the Google Drive so here as a sample a YouTube video I will be giving one of the videos from Geek Avenue the build document question answering app on extremely so this is the 25 24 minutes length YouTube video so I'll be using this in this particular tutorial so as the destination I am giving the same directory with a DOT and then I can set a name for this video as well so because it is getting saved in our Google Drive so next we can call this extract and save a video function here we Define so basically from this it will extract the video and convert it into audio and save it in our local Drive so here if you expand this file section in the collab you can see the corresponding MP3 file uh is saved in your same directory that you are using the Google collab all right next we can move to the transcribe section so here I will be using these precautions as well to avoid any overflows in the memory in collab environment so I will be doing the garbage collection stuff and then here I will be using this loaded model and then where I have just need to do is simply give the file name MP3 file name and then simply call The Whisper model so with this it will load this MP3 file and then it will do the transcription and then it will be uh emitted to the this variable result next I will move to uh chunk clips so with this basically we will chunk our YouTube video into smaller clips so here the idea is when we do question answering give references on which part of the video contain the answer to the provided question so that uh the person can re-watch that part if he wants so first we will get uh the segments from the transcription output from whisper like this next we can create a function like this chant Clips so basically it requires two inputs the first one is the transcription segments from the whisper and next it needs the clip size so basically this is the size of the chunk or the clip that we are providing so normally in whisper it annotates each phrase with the start time and the in time so we can use this for the Chunk in the video clip so here as the chunking delimiter we will use the number of phrases as given as the clip size so here I am creating a two lists the first one is text this is to hold text from each of the clip segment and this is to hold the time duration or the period for each of the clip segment next I can the slice the data frame with the given clip size or the given number of phrases and then create set of set of sub data frames next I will go through each of these sub data frames and then I am doing two things the first I will build a complete text from the uh clip chunk by joining all the text in the Raw between like this next I will create in the source so for this I will be using the first and then the last row of this sub data frame and then from those first and last Pros I will from the first row I will be using the start time frame and from the last data frame I will be using the ending time frame and these times are in seconds so I will be converting I will be dividing it with the 60 to convert it into minutes so I'll be also rounding it to get to two decimal places and like this I will be I can create the time stamp for the given uh video segment next I will append these texts as well as the sources to these two lists and I will emit those to list so basically with this way I can build set of video segments along with their time stamps so here I can call this created uh python function like this so I will set the clip size to 50 so the here the 50 means I am segmenting my video clip with from every 50 phases so next I can get the documents from the first output list from the chunks and next I can get the sources as well so for example this is how it looks live so this is the first part so the first part contain from 0 to 5.48 minutes and next uh we have this segment this segment likewise so our entire video is uh something around 24 minutes and it is split into five segments like this all good now we have extracted all our documents as well as our sources so we can do the regular steps that we are doing basically we can do the open AI key here and initiate in the embedding extractor instask also we can initiate the vector store so here with chroma from text we can give the documents embeddings as well as sources as the metadatas and next we can view the model name which is the chat GPD model and then I will also initiate the retriever so that we can give how many uh segments that need to be used when it does the question answering so always you can change these parameters and use your own parameters all right next we can initiate our model so here I'll be using the red view q a with sources chain and uh I will give the llm I created as the as well as the retriever so uh let's move to some Q and A so first I am asking what is this video about and then uh if I run this so this is the question the answer I'm getting it says this video is about creating an end-to-end uh extremely duplication so here there's a small mistake as well it should be string lit application that can communicate with multiple documents using streamless land chain and chroma DB and openness so this is pretty good so that's what basically I am doing with this YouTube tutorial also it gives the referred clip segments so it has used the first segment which is the introduction so and also it uh takes information from the end dot as well so this is pretty good and if you basically what this video is all about and next I am asking list down all the steps building this app so here also it gives all the almost all the steps uh that requires to build this application aspect so that's it from this video so you can cry out this method to do Q and A with uh YouTube videos Etc so you can also uh not uh limiting to the YouTube videos you can use it for any other video MP3 Etc so with MP3s or other videos you can amend these segments with the MP3 it's basically we can skip this above segments and load the MP3 file from the disk itself and use uh the code from here on verse so you can use this for the lengthy lectures or the podcast Etc and do Q and A and get the answers also referring to the which segments that these answers are referring from so that's it for this video and thank you for watching and please make sure to subscribe me so see you from the next video